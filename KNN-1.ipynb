{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae4d0ec-4823-4f01-8277-536461e80422",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Ans.The K-Nearest Neighbors (KNN) algorithm is a machine learning approach that classifies data points based on their similarity to neighboring points. It finds the K closest data points to a new point and assigns the most common label among them as the prediction for the new point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe792a3d-c3c4-4b64-8505-2a2ab3a8de67",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "\n",
    "Choosing the value of K in K-Nearest Neighbors (KNN) requires careful consideration. The selection process often involves experimentation and evaluation. Here are a few common approaches to choose K:\n",
    "\n",
    "1. Cross-validation: Split the data into training and validation sets. Evaluate the performance of KNN with different K values and select the K that yields the best performance on the validation set.\n",
    "\n",
    "2. Rule of thumb: A simple rule is to choose K as an odd number to avoid ties in the voting process. The square root of the number of samples in the training set is sometimes used as a starting point for K.\n",
    "\n",
    "3. Domain knowledge: Consider the characteristics of the problem domain. Some datasets may have specific patterns or properties that can guide the choice of K. For example, if classes are well-separated, a smaller K might be suitable.\n",
    "\n",
    "4. Grid search: Define a range of K values and evaluate the algorithm's performance using a validation set or cross-validation. Select the K that achieves the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d887bed9-95bc-42fb-9737-af8656f4bdeb",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Ans. KNN Classifier: The KNN classifier is used for classification tasks. It assigns a class label to a new data point based on the majority vote of its K nearest neighbors. The class label assigned to the new point is determined by the most frequent class among the K neighbors.\n",
    "\n",
    "KNN Regressor: The KNN regressor, on the other hand, is used for regression tasks. It predicts a continuous numeric value for a new data point based on the average (or weighted average) of the target values of its K nearest neighbors. The predicted value for the new point is calculated by taking the mean or weighted mean of the target values of the K neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca34f4-5ea0-41f4-bbe6-eb83806bcc58",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "ans.   Classification:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances\n",
    "\n",
    "Precision: The ratio of true positives to the sum of true positives and false positives.\n",
    "\n",
    "Recall: The ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "F1 score: The harmonic mean of precision and recall.\n",
    "\n",
    "Regression:\n",
    "\n",
    "Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values.\n",
    "\n",
    "Mean Squared Error (MSE): The average squared difference between the predicted and actual values.\n",
    "\n",
    "Root Mean Squared Error (RMSE): The square root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491baf6-27b0-4f01-9898-00f60976a49e",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Ans.The curse of dimensionality in KNN refers to the challenges and limitations that arise when working with high-dimensional data. As the number of dimensions increases, the available data becomes sparse, leading to a reduction in the effectiveness of the KNN algorithm. With high dimensionality, the distance between data points becomes less informative, making it harder to identify nearest neighbors accurately. This can result in degraded performance, increased computational complexity, and the need for larger training datasets. The curse of dimensionality highlights the importance of dimensionality reduction techniques and careful feature selection when applying KNN to high-dimensional data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be492c94-4141-454d-94ff-f19b73659902",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Ans. Deletion: Remove data instances that contain missing values. This can be done if the amount of missing data is relatively small and doesn't significantly impact the overall dataset size.\n",
    "\n",
    "Imputation: Replace missing values with estimated values. In KNN, the missing values can be imputed by taking the average or median value of the available values from the nearest neighbors. This imputation is performed separately for each missing feature.\n",
    "\n",
    "Weighted KNN: Modify the KNN algorithm to consider weights based on the similarity of instances. The weights can be computed by taking into account the available values when calculating distances between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdedc37-402b-4955-9cec-c802dee3c6f7",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "ans. KNN Classifier:\n",
    "\n",
    "Performance: The KNN classifier is effective for classification tasks, where the goal is to assign data points to predefined classes or categories. It works well when there is a clear separation between classes and the decision boundaries are relatively simple.\n",
    "\n",
    "Evaluation Metrics: Accuracy, precision, recall, and F1 score are commonly used to measure the performance of a KNN classifier.\n",
    "Appropriate Problems: KNN classifier is suitable for problems such as image recognition, text categorization, sentiment analysis, and disease diagnosis, where the task is to classify data into distinct classes.\n",
    "\n",
    "KNN Regressor:\n",
    "\n",
    "Performance: The KNN regressor is useful for regression tasks, where the objective is to predict continuous numeric values. It is effective when the underlying relationship between features and target values is nonlinear or when there is no specific functional form assumed.\n",
    "\n",
    "Evaluation Metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) are commonly used to evaluate the performance of a KNN regressor.\n",
    "Appropriate Problems: KNN regressor is suitable for problems such as house price prediction, stock market forecasting, and demand estimation, where the target variable is a continuous value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2ec2c-c784-4a50-b6c6-fbee740bbe4e",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "ANS. The K-Nearest Neighbors (KNN) algorithm has its strengths and weaknesses for both classification and regression tasks:\n",
    "\n",
    "Strengths of KNN:\n",
    "\n",
    "Simplicity: KNN is straightforward and easy to understand, making it a good starting point for many machine learning tasks.\n",
    "Non-parametric: KNN does not make strong assumptions about the underlying data distribution, making it flexible and suitable for a wide range of problems.\n",
    "Adaptability to new data: KNN can easily incorporate new training instances without retraining the model, allowing it to adapt to changing data.\n",
    "Weaknesses of KNN:\n",
    "\n",
    "Computational complexity: As the dataset grows, the computational cost of searching for nearest neighbors increases, making KNN inefficient for large datasets.\n",
    "Sensitivity to irrelevant features: KNN treats all features equally, which means irrelevant or noisy features can negatively impact its performance.\n",
    "Determining the optimal value of K: Choosing the appropriate value for K can be challenging and may require tuning or experimentation.\n",
    "Addressing the weaknesses:\n",
    "\n",
    "Dimensionality reduction: Applying dimensionality reduction techniques, such as Principal Component Analysis (PCA), can mitigate the curse of dimensionality and reduce computational complexity.\n",
    "Feature selection: Careful feature selection can help exclude irrelevant or noisy features, improving the performance of KNN.\n",
    "Distance weighting: Assigning weights to the neighbors based on their distance can give more importance to closer neighbors and reduce the impact of outliers.\n",
    "Cross-validation and parameter tuning: Employing cross-validation techniques and grid search can help find the optimal value of K and improve the overall performance of KNN.\n",
    "By addressing these weaknesses through appropriate preprocessing, feature selection, and parameter tuning, the performance of the KNN algorithm for both classification and regression tasks can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f6204-19db-4734-8ff2-7e15b52e16ed",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "ans. Euclidean Distance (L2 norm):\n",
    "\n",
    "Calculation: Euclidean distance is calculated as the square root of the sum of squared differences between corresponding feature values.\n",
    "\n",
    "Interpretation: It represents the straight-line or Euclidean distance between two points in a Cartesian plane.\n",
    "\n",
    "Characteristics: Euclidean distance considers the magnitude and direction of the differences between features. It is influenced by the absolute differences in all dimensions.\n",
    "\n",
    "Manhattan Distance (L1 norm):\n",
    "\n",
    "Calculation: Manhattan distance is computed as the sum of absolute differences between corresponding feature values.\n",
    "\n",
    "Interpretation: It represents the distance between two points measured along the axes of a grid-like structure.\n",
    "\n",
    "Characteristics: Manhattan distance focuses on the absolute differences in each dimension independently. It is less sensitive to outliers and the influence of individual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07abef-afc6-43fb-83b0-b01f8005c2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
