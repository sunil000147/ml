{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561f0e54-4ba3-47b3-9f5b-a0e12193e166",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "ans.  formula for a linear svm:\n",
    "    \n",
    "f(x) =w^T + x\n",
    "\n",
    "where,\n",
    "\n",
    "w= weight of vector\n",
    "\n",
    "t=trasnpose\n",
    "\n",
    "x=input vector\n",
    "\n",
    "f(x)=output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c05520-bf33-4957-8ae2-4cde8566f57a",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM? \n",
    "\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to maximize the margin between the classes while minimizing the classification errors. It can be expressed as:\n",
    "\n",
    "minimize: 1/2 * ||w||^2 + C * Σξ_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0e92f-dd02-4282-a123-b3e4d8efa6a7",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "ans.\n",
    "\n",
    "The kernel trick is a technique used in Support Vector Machines (SVMs) to handle non-linearly separable data without explicitly mapping the data to a higher-dimensional feature space. It allows SVMs to efficiently compute the decision boundary in the original input space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0d937-5c0d-4f3b-a81e-6ef855fabf79",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example \n",
    "\n",
    "\n",
    "The support vectors in SVMs are the data points from the training set that lie closest to the decision boundary. They play a critical role in defining the decision boundary and making predictions. For example, consider a binary classification problem with blue circles and red squares. The support vectors are the points closest to the decision boundary, circled in green. They determine the position and orientation of the decision boundary. During prediction, SVM only needs to consider the support vectors, making it computationally efficient. Support vectors ensure good generalization and robustness to outliers, allowing SVMs to handle complex, non-linear decision boundaries while avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41159b2f-2b0c-4ee2-9a7c-7dc13f2f54ca",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "\n",
    " Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM using examples and graphs.\n",
    "\n",
    "Hyperplane:\n",
    "\n",
    "In SVM, the hyperplane is a decision boundary that separates data points of different classes. In a binary classification problem, the hyperplane is a line in a 2D space or a plane in a 3D space. The hyperplane can be defined as w * x + b = 0, where w is the weight vector, x represents the input vector, and b is the bias term or intercept. The hyperplane separates the feature space into two regions corresponding to different classes.\n",
    "Example: Consider a 2D space with two classes, represented by blue circles and red squares. The hyperplane is a straight line that separates the two classes, as shown in the graph below:\n",
    "\n",
    "\n",
    "Marginal Plane:\n",
    "\n",
    "The marginal plane refers to the parallel planes that run parallel to the hyperplane and define the margin in SVM. The margin is the region between these marginal planes that contains the support vectors. In a binary classification problem, there are two marginal planes: one on each side of the hyperplane. The distance between the marginal planes is the margin, and it is maximized in SVM to achieve a better separation of classes.\n",
    "Example: Continuing with the previous example, the marginal planes are depicted as dashed lines parallel to the hyperplane in the graph below:\n",
    "\n",
    "\n",
    "\n",
    "Hard Margin:\n",
    "\n",
    "In SVM, a hard margin refers to the scenario where the margin is maximized, and there is no tolerance for misclassification. It means that the SVM aims to find a hyperplane that perfectly separates the classes without allowing any training examples to fall within the margin or be misclassified. Hard margin SVM is suitable when the data is perfectly separable.\n",
    "Example: In a 2D space with perfectly separable classes, the hard margin SVM finds a hyperplane that separates the classes without any margin violations, as shown below:\n",
    "\n",
    "\n",
    "Soft Margin:\n",
    "\n",
    "Soft margin SVM is used when the data is not perfectly separable or when allowing some margin violations and misclassifications improves generalization. In soft margin SVM, a regularization parameter (often denoted as C) is introduced to balance the trade-off between maximizing the margin and minimizing the classification errors. A higher value of C allows for fewer margin violations and misclassifications, resulting in a narrower margin. Conversely, a lower value of C allows for more tolerance of margin violations and misclassifications, resulting in a wider margin."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
