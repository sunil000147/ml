{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feda8d6d-95aa-4e65-bd7a-b170039b3b7d",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "ans. Gradient Boosting Regression is a machine learning technique that combines multiple weak models to create a strong predictor for numerical values. It trains models sequentially, where each new model corrects the mistakes made by previous models. It does this by focusing on the data points that were poorly predicted before. It's like a team of specialists working together, where each member improves upon the mistakes of the previous member. By repeatedly adjusting the models, it gradually learns the patterns in the data and makes accurate predictions. It's powerful and widely used for solving regression problems in various domains.\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In gradient boosting, a weak learner is a simple model, like a small and shallow decision tree, that performs slightly better than random guessing. Weak learners are combined sequentially to create a stronger model that can capture complex patterns and make accurate predictions.\n",
    "\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to sequentially add weak learners to the ensemble, with each learner focusing on correcting the mistakes made by the previous learners. By combining the predictions of these weak learners, the algorithm gradually improves its predictive performance and creates a strong overall model.\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding them to the ensemble. Each weak learner is trained to correct the mistakes made by the previous learners. The algorithm combines the predictions of all the weak learners to make accurate predictions as a whole.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "ans. The mathematical intuition behind the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. Initialize with an initial prediction: Start by setting an initial prediction, usually the mean value of the target variable.\n",
    "\n",
    "2. Calculate residuals: Compute the difference between the actual target values and the initial prediction. These differences are the residuals or errors.\n",
    "\n",
    "3. Train a weak learner: Fit a weak learner, such as a decision tree with limited depth, to the residuals. The learner tries to capture the patterns in the residuals and make predictions that reduce the errors.\n",
    "\n",
    "4. Update the ensemble: Add the trained weak learner to the ensemble of learners, with a weight indicating its contribution to the final prediction.\n",
    "\n",
    "5. Adjust residuals: Update the residuals by subtracting the predictions made by the latest weak learner. This process focuses on capturing the remaining errors that were not corrected by the earlier learners.\n",
    "\n",
    "6. Repeat steps 3-5: Iterate the process by training additional weak learners on the updated residuals, adding them to the ensemble, and adjusting the residuals.\n",
    "\n",
    "7. Generate the final prediction: Combine the predictions from all the weak learners in the ensemble, usually by summing them, to produce the final prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da3f126-94f3-465e-bc07-ecf703faa454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 5438462547.667847\n",
      "R-squared: -185466315.17806283\n"
     ]
    }
   ],
   "source": [
    "'''Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.'''\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# define the gradient boosting algorithm class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "        self.intercept = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Initialize the intercept with the mean of the target variable\n",
    "        self.intercept = np.mean(y)\n",
    "        y_pred = np.full_like(y, self.intercept)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute the negative gradient (residuals) with respect to the current predictions\n",
    "            residuals = -(y - y_pred)\n",
    "            \n",
    "            # Train a decision tree regressor on the negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=1)\n",
    "            tree.fit(X, residuals)\n",
    "            \n",
    "            # Update the predictions by adding the current tree's predictions scaled by the learning rate\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "            \n",
    "            # Store the trained tree in the list of estimators\n",
    "            self.estimators.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Initialize the predictions with the intercept\n",
    "        y_pred = np.full(X.shape[0], self.intercept)\n",
    "        \n",
    "        for tree in self.estimators:\n",
    "            # Add the predictions from each tree to the current predictions\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "# Create a toy regression dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X[:, 0] + np.random.randn(100)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "X_test, y_test = X[split_idx:], y[split_idx:]\n",
    "\n",
    "# Train the gradient boosting model\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Calculate mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a73fb9-e33f-424c-9e2f-f0c8ad4eabb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
